{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a __tensorflow__ neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for tensorflow, but you will find it easy to adapt it to almost any python-based deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: ../xvfb: No such file or directory\n",
      "env: DISPLAY=:1\n"
     ]
    }
   ],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if os.environ.get(\"DISPLAY\") is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime\n",
      "/usr/local/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (hashtable as _hashtable,\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos, lib\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import hashing, tslib\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (lib, index as libindex, tslib as libts,\n",
      "/usr/local/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.tslibs.offsets as liboffsets\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/ops.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos as libalgos, ops as libops\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/indexes/interval.py:32: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs.interval import (\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/internals.py:14: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import internals as libinternals\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/sparse/array.py:33: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.sparse as splib\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/window.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.window as _window\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/groupby/groupby.py:68: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (lib, reduction,\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/reshape/reshape.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos as _algos, reshape as _reshape\n",
      "/usr/local/lib/python2.7/site-packages/pandas/io/parsers.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.parsers as parsers\n",
      "/usr/local/lib/python2.7/site-packages/pandas/io/pytables.py:50: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos, lib, writers as libwriters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khairunn/Documents/DAT203/intro-to-dl/reinforcementLearning/week_3/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11d71b550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE4NJREFUeJzt3X+s3XWd5/HnqyBEZan8kKItUhYZBjZOWiYWZsxmDjtG\nQXeHzWYRnNmgLkQmw+yYcXZDcbPpdZ1khklg1bgEdZDAZBQYdlU0yq/FY4IZfizQoZtCrTODtJAW\nIj9c1EVK3/vH/bYe2lvvufee09Pez/ORnPR73uf74/1JT1/n08+5555UFZKkNiyZdAOSpP3H0Jek\nhhj6ktQQQ1+SGmLoS1JDDH1JasjYQj/JOUkeT/L9JJeP6zqSpOFlHD+nn2QJ8H3gt4GngQeBC6vq\n8ZFfTJI0tHHN9NcAm6vqh1X1CnATcN6YriVJGtK4Qn85sGXg/tauJkmaoHGFfmao+fseJGnCDh3T\nebcCbxu4v4Lptf3dkvgiIEnzUFUzTayHMq6Z/oPA25OcmOQw4ELgtj13qqpFe1u3bt3Ee3B8jq/F\n8S3msVUtfK48lpl+Vb2a5A+BO5l+Ybmuqh4bx7UkScMb1/IOVXU7cOq4zi9Jmjs/kTsmvV5v0i2M\nleM7uC3m8S3msY3CWD6cNdSFk5rUtSXpYJWEOgDfyJUkHYAMfUlqiKEvSQ0x9CWpIYa+JDXE0Jek\nhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQxb0HblJ\nngBeBHYCr1TVmiRHATcDJwJPAB+oqhcX2KckaQQWOtPfCfSqanVVrelqa4G7q+pU4B7gigVeQ5I0\nIgsN/cxwjvOAG7rtG4B/vcBrSJJGZKGhX8AdSR5McklXW1ZV2wGqahvw5gVeQ5I0Igta0wd+s6q2\nJXkzcGeSTUy/EEiSDkALCv1uJk9VPZvka8AaYHuSZVW1PcnxwDP7On5qamr3dq/Xo9frLaQdSVp0\n+v0+/X5/ZOdL1fwm5kneACypqpeSvBG4E/gk8NvAc1V1ZZLLgaOqau0Mx9d8ry1JrUpCVWXexy8g\n9E8Cvsr0cs6hwF9X1Z8nORq4BTgBeBI4v6pemOF4Q1+S5mhiob9Qhr4kzd1CQ99P5EpSQwx9SWqI\noS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6\nktQQQ1+SGmLoS1JDDH1JaoihL0kNmTX0k1yXZHuSRwdqRyW5M8mmJHckWTrw2GeTbE6yPsmqcTUu\nSZq7YWb61wPv3aO2Fri7qk4F7gGuAEhyLnByVZ0CXApcO8JeJUkLNGvoV9W9wPN7lM8Dbui2b+ju\n76rf2B13P7A0ybLRtCpJWqj5rukfV1XbAapqG3BcV18ObBnY76muJkk6AIz6jdzMUKsRX0OSNE+H\nzvO47UmWVdX2JMcDz3T1rcAJA/utAJ7e10mmpqZ2b/d6PXq93jzbkaTFqd/v0+/3R3a+VM0+EU+y\nEvhGVb2ju38l8FxVXZlkLfCmqlqb5H3AZVX1/iRnAZ+uqrP2cc4a5tqSpF9IQlXNtKoy3PGzBW+S\nLwM94BhgO7AO+BrwN0zP6p8Ezq+qF7r9PwecA/wE+EhVPbyP8xr6kjRHYw/9cTH0JWnuFhr6fiJX\nkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWp\nIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JasisoZ/kuiTbkzw6UFuXZGuSh7vbOQOPXZFkc5LH\nkrxnXI1LkuZumJn+9cB7Z6hfXVVndLfbAZKcBnwAOA04F7gmyby/wFeSNFqzhn5V3Qs8P8NDM4X5\necBNVbWjqp4ANgNrFtShJGlkFrKmf1mS9Un+MsnSrrYc2DKwz1NdTZJ0AJhv6F8DnFxVq4BtwFVd\nfabZf83zGpKkETt0PgdV1bMDd78IfKPb3gqcMPDYCuDpfZ1nampq93av16PX682nHUlatPr9Pv1+\nf2TnS9XsE/EkK4FvVNU7uvvHV9W2bvuPgXdW1e8mOR34a+BMppd17gJOqRkukmSmsiTpl0hCVc37\nB2Rmnekn+TLQA45J8iSwDjg7ySpgJ/AEcClAVW1McguwEXgF+AOTXZIOHEPN9MdyYWf6kjRnC53p\n+4lcSWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLe9h466d4+qFvTroNaSzm9bt3pMXk\noS9culftjcet3P+NSPuBM31JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqI\noS9JDZk19JOsSHJPko1JNiT5o65+VJI7k2xKckeSpQPHfDbJ5iTruy9Qlw5Yh/2TY/eqvfLTFyfQ\niTR+w8z0dwAfr6rTgd8ALkvyq8Ba4O6qOhW4B7gCIMm5wMlVdQpwKXDtWDqXRuTNp/3zvWovPrlh\nAp1I4zdr6FfVtqpa322/BDwGrADOA27odruhu0/3543d/vcDS5MsG3HfkqR5mNOafpKVwCrgPmBZ\nVW2H6RcG4Lhut+XAloHDnupqkqQJG/pXKyc5ArgV+FhVvZSk9rXrDLUZ952amtq93ev16PV6w7Yj\nSU3o9/v0+/2RnS9V+8rugZ2SQ4FvAt+uqs90tceAXlVtT3I88J2qOi3Jtd32zd1+jwO/tet/BQPn\nrGGuLY3btvW389QDX92r/usf/fwEupF+uSRU1UyT66EMu7zzJWDjrsDv3AZ8uNv+MPD1gfpFXXNn\nAS/sGfiSpMmYdXknybuA3wM2JHmE6aWaTwBXArck+ffAk8D5AFX1rSTvS/ID4CfAR8bVvCRpbmYN\n/ar6HnDIPh5+9z6O+cOFNCVJGg8/kStJDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlq\niKEvSQ0x9CWpIYa+mnf8qnNmrG+89VP7uRNp/Ax9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBD\nX5IaYuhLUkNmDf0kK5Lck2Rjkg1J/kNXX5dka5KHu9s5A8dckWRzkseSvGecA5AkDW/WL0YHdgAf\nr6r1SY4AHkpyV/fY1VV19eDOSU4DPgCcBqwA7k5ySlXVKBuXJM3drDP9qtpWVeu77ZeAx4Dl3cOZ\n4ZDzgJuqakdVPQFsBtaMpl1J0kLMaU0/yUpgFXB/V7osyfokf5lkaVdbDmwZOOwpfvEiIUmaoGGW\ndwDolnZuBT5WVS8luQb4r1VVSf4UuAq4hJln/zMu7UxNTe3e7vV69Hq94TuXpAb0+336/f7Izpdh\nltqTHAp8E/h2VX1mhsdPBL5RVb+WZC1QVXVl99jtwLqqun+PY1zm1wHjoS9culft9Uev4PR/+18m\n0I20b0moqpkm10MZdnnnS8DGwcBPcvzA4/8G+D/d9m3AhUkOS3IS8Hbggfk2KEkanVmXd5K8C/g9\nYEOSR5heqvkE8LtJVgE7gSeASwGqamOSW4CNwCvAHzill6QDw6yhX1XfAw6Z4aHbf8kxfwb82QL6\nkiSNgZ/IlaSGGPoS8Cv/8uN71X723FZ27nhlAt1I42PoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCX\npIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0JakhQ31z1lgu7DdnacwefvhhfvjDHw617+E//xHL\nXnxwr/qWY99DZbi50erVq1m5cuVcWpTmbKHfnDX0d+RKB5trrrmG6667bqh9f/1X3sLn/+Rf7VW/\n4IIL+PmOV4c6xxe/+EUuueSSOfUo7W+GvtR5tQ7hO89csPv+u5d9eYLdSOPhmr7U+fa2i/l/O4/Y\nfbt924fY6T8RLTI+oyXgZ68esVdtRx0OzHvpVDogzRr6SQ5Pcn+SR5JsSLKuq69Mcl+STUm+kuTQ\nrn5YkpuSbE7yt0neNu5BSAv16s+fm7G+6u3H7+dOpPGaNfSr6mXg7KpaDawCzk1yJnAlcFVVnQq8\nAFzcHXIx8FxVnQJ8GviLsXQujdCmLT9iCTteU1v6ume49o/PmVBH0ngM9UZuVf202zy8O6aAs4EP\ndvUbgHXA54Hzum2AW4HPjapZaZzOPf5L/N8dRwPwwU/9D4583Y+4ZsI9SaM2VOgnWQI8BJwM/Hfg\n74EXqmpnt8tWYHm3vRzYAlBVryZ5IcnRVbXX/58/+tGPLrB9ad/uvffeOe3/zt//wmvub5/j9W68\n8UYeeOCBOR4l7V/DzvR3AquTHAl8FThtpt26P/d85ysDj73GW9/61t3bvV6PXq83TDvSUC655BI2\nbdq036530UUX+XP6Grl+v0+/3x/Z+eb0c/pV9eMk3wXOAt6UZEn3grACeLrbbStwAvB0kkOAI6vq\n+ZnONzU1Ne/GJakFe06IP/nJTy7ofMP89M6xSZZ2268H3g1sBL4DnN/t9iHg6932bd19usfvWVCH\nkqSRGWam/xbghm5dfwlwc1V9K8ljwE1JPgU8Auz6vPt1wF8l2Qz8CLhwDH1LkuZh1tCvqg3AGTPU\n/xE4c4b6y8AHRtKdJGmk/ESuJDXE0Jekhhj6ktQQv0RFi9ZDDz3EE088sd+ud8YZZ3DSSSftt+up\nTQv9EhVDX5IOIgsNfZd3JKkhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+\nJDXE0Jekhhj6ktQQQ1+SGjLMF6MfnuT+JI8k2ZBkXVe/Psk/dPWHk/zawDGfTbI5yfokq8Y5AEnS\n8Ib5jtyXk5xdVT9NcgjwvSS3dw//x6r6n4P7JzkXOLmqTklyJnAtcNbIO5ckzdlQyztV9dNu83Cm\nXyh2dvdn+p3O5wE3dsfdDyxNsmyBfUqSRmCo0E+yJMkjwDbgrqp6sHvoT7slnKuSvK6rLQe2DBz+\nVFeTJE3YsDP9nVW1GlgBrElyOrC2qk4D3gkcA1ze7T7T7N+vyJKkA8Csa/qDqurHSb4LnFNVV3e1\nV5JcD/xJt9tW4ISBw1YAT890vqmpqd3bvV6PXq83l3YkadHr9/v0+/2RnW/W78hNcizwSlW9mOT1\nwB3AnwMPV9W2JAGuBn5WVZ9I8j7gsqp6f5KzgE9X1V5v5PoduZI0dwv9jtxhZvpvAW5IsoTp5aCb\nq+pbSf5X94IQYD3w+wDdY+9L8gPgJ8BH5tucJGm0Zp3pj+3CzvQlac4WOtP3E7mS1BBDX5IaYuhL\nUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1\nxNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoT8m/X5/0i2MleM7uC3m8S3msY2CoT8mi/2J5/gObot5\nfIt5bKNg6EtSQwx9SWpIqmoyF04mc2FJOshVVeZ77MRCX5K0/7m8I0kNMfQlqSETCf0k5yR5PMn3\nk1w+iR4WKsl1SbYneXSgdlSSO5NsSnJHkqUDj302yeYk65OsmkzXw0myIsk9STYm2ZDkj7r6Yhnf\n4UnuT/JIN751XX1lkvu68X0lyaFd/bAkN3Xj+9skb5vsCIaTZEmSh5Pc1t1fNONL8kSSv+v+Dh/o\naovi+Tlu+z30kywBPge8F/hnwAeT/Or+7mMErmd6DIPWAndX1anAPcAVAEnOBU6uqlOAS4Fr92ej\n87AD+HhVnQ78BnBZ93e0KMZXVS8DZ1fVamAVcG6SM4Ergau68b0AXNwdcjHwXDe+TwN/MYG25+Nj\nwMaB+4tpfDuBXlWtrqo1XW1RPD/Hrqr26w04C/j2wP21wOX7u48RjeVE4NGB+48Dy7rt44HHuu1r\ngQsG9nts134Hww34GvDuxTg+4A3A/wbWAM8AS7r67ucpcDtwZrd9CPDspPseYlwrgLuAHnBbV3t2\nEY3vH4Fj9qgtuufnOG6TWN5ZDmwZuL+1qy0Gx1XVdoCq2gYc19X3HPNTHCRjTrKS6dnwfUz/Q1kU\n4+uWPh4BtjEdjn8PvFBVO7tdBp+Xu8dXVa8CLyQ5ej+3PFf/DfhPQAEkOQZ4fhGNr4A7kjyY5JKu\ntmien+N06ASuOdPPly72nxs9KMec5AjgVuBjVfXSL/lsxUE3vi78Vic5EvgqcNpMu3V/7jm+cACP\nL8n7ge1VtT5Jb1eZvcdxUI6v85tVtS3Jm4E7k2xi3z0fdM/PcZrETH8rMPhG0Qrg6Qn0MQ7bkywD\nSHI808sFMD3mEwb2O+DH3L3JdyvwV1X19a68aMa3S1X9GPgu08sdb+rec4LXjmH3+JIcAhxZVc/v\n717n4F3A7yT5B+ArwL9geq1+6SIZ366ZPFX1LNPLj2tYhM/PcZhE6D8IvD3JiUkOAy4EbptAH6Ow\n5+zpNuDD3faHga8P1C8CSHIW08sI2/dPi/P2JWBjVX1moLYoxpfk2F0/2ZHk9Uy/X7ER+A5wfrfb\nh3jt+D7UbZ/P9JuEB6yq+kRVva2q/inT/77uqap/xyIZX5I3dP8LJckbgfcAG1gkz8+xm9CbMOcA\nm4DNwNpJv7ExzzF8menZwsvAk8BHgKOAu7ux3QW8aWD/zwE/AP4OOGPS/c8ytncBrwLrgUeAh7u/\ns6MXyfje0Y1pPfAo8J+7+knA/cD3gZuB13X1w4FbuufrfcDKSY9hDmP9LX7xRu6iGF83jl3PzQ27\nMmSxPD/HffPXMEhSQ/xEriQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakh/x9wYjA+\n+VZMpAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119076610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "# print(env.action_space.n)\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "To train a neural network policy one must have a neural network policy. Let's build it.\n",
    "\n",
    "\n",
    "Since we're working with a pre-extracted features (cart positions, angles and velocities), we don't need a complicated network yet. In fact, let's build something like this for starters:\n",
    "\n",
    "![img](https://s14.postimg.org/uzay2q5rl/qlearning_scheme.png)\n",
    "\n",
    "For your first run, please only use linear layers (L.Dense) and activations. Stuff like batch normalization or dropout may ruin everything if used haphazardly. \n",
    "\n",
    "Also please avoid using nonlinearities like sigmoid & tanh: agent's observations are not normalized so sigmoids may become saturated from init.\n",
    "\n",
    "Ideally you should start small with maybe 1-2 hidden layers with < 200 neurons and then increase network size if agent doesn't beat the target score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:34: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:43: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n",
      "/usr/local/lib/python2.7/site-packages/h5py/_hl/group.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n",
      "/usr/local/lib/python2.7/site-packages/scipy/sparse/lil.py:19: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _csparsetools\n",
      "/usr/local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\\\n",
      "/usr/local/lib/python2.7/site-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._tools import csgraph_to_dense, csgraph_from_dense,\\\n",
      "/usr/local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:167: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._traversal import breadth_first_order, depth_first_order, \\\n",
      "/usr/local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:169: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._min_spanning_tree import minimum_spanning_tree\n",
      "/usr/local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:170: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._reordering import reverse_cuthill_mckee, maximum_bipartite_matching, \\\n",
      "/usr/local/lib/python2.7/site-packages/scipy/linalg/basic.py:17: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._solve_toeplitz import levinson\n",
      "/usr/local/lib/python2.7/site-packages/scipy/linalg/__init__.py:207: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._decomp_update import *\n",
      "/usr/local/lib/python2.7/site-packages/scipy/special/__init__.py:640: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ufuncs import *\n",
      "/usr/local/lib/python2.7/site-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n",
      "/usr/local/lib/python2.7/site-packages/scipy/interpolate/_bsplines.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _bspl\n",
      "/usr/local/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .ckdtree import *\n",
      "/usr/local/lib/python2.7/site-packages/scipy/spatial/__init__.py:96: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .qhull import *\n",
      "/usr/local/lib/python2.7/site-packages/scipy/spatial/_spherical_voronoi.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _voronoi\n",
      "/usr/local/lib/python2.7/site-packages/scipy/spatial/distance.py:122: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _hausdorff\n",
      "/usr/local/lib/python2.7/site-packages/scipy/ndimage/measurements.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _ni_label\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = keras.models.Sequential()\n",
    "network.add(L.InputLayer(state_dim))\n",
    "\n",
    "# let's create a network for approximate q-learning following guidelines above\n",
    "\n",
    "network.add(L.Dense(200, activation='relu'))\n",
    "\n",
    "network.add(L.Dense(200, activation='relu'))\n",
    "\n",
    "network.add(L.Dense(n_actions, activation='linear'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "\n",
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    \n",
    "    q_values = network.predict(state[None])[0]\n",
    "    \n",
    "    ###YOUR CODE    \n",
    "    rand = random.uniform(0,1)\n",
    "    \n",
    "    if rand < epsilon:\n",
    "        action = np.random.choice(n_actions, 1)[0]\n",
    "    else:\n",
    "        action = np.argmax(q_values)\n",
    "        \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "e=0.0 tests passed\n",
      "26.0\n",
      "e=0.1 tests passed\n",
      "37.0\n",
      "e=0.5 tests passed\n",
      "5.0\n",
      "e=1.0 tests passed\n"
     ]
    }
   ],
   "source": [
    "assert network.output_shape == (None, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert network.layers[-1].activation == keras.activations.linear, \"please make sure you predict q-values without nonlinearity\"\n",
    "\n",
    "# test epsilon-greedy exploration\n",
    "s = env.reset()\n",
    "assert np.shape(get_action(s)) == (), \"please return just one action (integer)\"\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "    best_action = state_frequencies.argmax()\n",
    "    if i % 1000:\n",
    "        print(abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)))\n",
    "    # assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
    "    # for other_action in range(n_actions):\n",
    "        #if other_action != best_action:\n",
    "            #assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
    "    print('e=%.1f tests passed'%eps)\n",
    "    # plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning via gradient descent\n",
    "\n",
    "We shall now train our agent's Q-function by minimizing the TD loss:\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$\n",
    "\n",
    "\n",
    "Where\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "The tricky part is with  $Q_{-}(s',a')$. From an engineering standpoint, it's the same as $Q_{\\theta}$ - the output of your neural network policy. However, when doing gradient descent, __we won't propagate gradients through it__ to make training more stable (see lectures).\n",
    "\n",
    "To do so, we shall use `tf.stop_gradient` function which basically says \"consider this thing constant when doingbackprop\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
    "states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "actions_ph = tf.placeholder('int32', shape=[None])\n",
    "rewards_ph = tf.placeholder('float32', shape=[None])\n",
    "next_states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "is_done_ph = tf.placeholder('bool', shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get q-values for all actions in current states\n",
    "predicted_qvalues = network(states_ph)\n",
    "\n",
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, n_actions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "# compute q-values for all actions in next states\n",
    "predicted_next_qvalues = network(next_states_ph)\n",
    "\n",
    "# compute V*(next_states) using predicted next q-values\n",
    "next_state_values = tf.reduce_max(predicted_next_qvalues, axis = 1)\n",
    "\n",
    "# compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "target_qvalues_for_actions = rewards_ph + gamma * next_state_values\n",
    "\n",
    "# at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean squared error loss to minimize\n",
    "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# training function that resembles agent.update(state, action, reward, next_state) from tabular agent\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"make sure you update q-values for chosen actions and not just all actions\"\n",
    "assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"make sure you don't propagate gradient w.r.t. Q_(s',a')\"\n",
    "assert predicted_next_qvalues.shape.ndims == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "assert next_state_values.shape.ndims == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "assert target_qvalues_for_actions.shape.ndims == 1, \"there's something wrong with target q-values, they must be a vector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)       \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        if train:\n",
    "            sess.run(train_step,{\n",
    "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
    "                next_states_ph: [next_s], is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = 14.420\tepsilon = 0.500\n",
      "epoch #1\tmean reward = 14.250\tepsilon = 0.495\n",
      "epoch #2\tmean reward = 13.970\tepsilon = 0.490\n",
      "epoch #3\tmean reward = 13.410\tepsilon = 0.485\n",
      "epoch #4\tmean reward = 15.450\tepsilon = 0.480\n",
      "epoch #5\tmean reward = 15.640\tepsilon = 0.475\n",
      "epoch #6\tmean reward = 25.440\tepsilon = 0.471\n",
      "epoch #7\tmean reward = 27.630\tepsilon = 0.466\n",
      "epoch #8\tmean reward = 31.630\tepsilon = 0.461\n",
      "epoch #9\tmean reward = 42.930\tepsilon = 0.457\n",
      "epoch #10\tmean reward = 43.990\tepsilon = 0.452\n",
      "epoch #11\tmean reward = 54.700\tepsilon = 0.448\n",
      "epoch #12\tmean reward = 59.180\tepsilon = 0.443\n",
      "epoch #13\tmean reward = 113.680\tepsilon = 0.439\n",
      "epoch #14\tmean reward = 133.270\tepsilon = 0.434\n",
      "epoch #15\tmean reward = 145.850\tepsilon = 0.430\n",
      "epoch #16\tmean reward = 170.830\tepsilon = 0.426\n",
      "epoch #17\tmean reward = 153.360\tepsilon = 0.421\n",
      "epoch #18\tmean reward = 209.200\tepsilon = 0.417\n",
      "epoch #19\tmean reward = 193.630\tepsilon = 0.413\n",
      "epoch #20\tmean reward = 204.500\tepsilon = 0.409\n",
      "epoch #21\tmean reward = 210.750\tepsilon = 0.405\n",
      "epoch #22\tmean reward = 269.310\tepsilon = 0.401\n",
      "epoch #23\tmean reward = 318.900\tepsilon = 0.397\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "    \n",
    "    epsilon *= 0.99\n",
    "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "    \n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret results\n",
    "\n",
    "\n",
    "Welcome to the f.. world of deep f...n reinforcement learning. Don't expect agent's reward to smoothly go up. Hope for it to go increase eventually. If it deems you worthy.\n",
    "\n",
    "Seriously though,\n",
    "* __ mean reward__ is the average reward per game. For a correct implementation it may stay low for some 10 epochs, then start growing while oscilating insanely and converges by ~50-100 steps depending on the network architecture. \n",
    "* If it never reaches target score by the end of for loop, try increasing the number of hidden neurons or look at the epsilon.\n",
    "* __ epsilon__ - agent's willingness to explore. If you see that agent's already at < 0.01 epsilon before it's is at least 200, just reset it back to 0.1 - 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record videos\n",
    "\n",
    "As usual, we now use `gym.wrappers.Monitor` to record a video of our agent playing the game. Unlike our previous attempts with state binarization, this time we expect our agent to act ~~(or fail)~~ more smoothly since there's no more binarization error at play.\n",
    "\n",
    "As you already did with tabular q-learning, we set epsilon=0 for final evaluation to prevent agent from exploring himself to death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "sessions = [generate_session(epsilon=0, train=False) for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submit to coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "import grading \n",
    "\n",
    "def submit_cartpole(generate_session, email, token):\n",
    "    sessions = [generate_session() for _ in range(100)]\n",
    "    session_rewards = np.array(sessions) \n",
    "    grader = grading.Grader(\"RDofv-QXEeeaGw6kpIOf3g\")\n",
    "    grader.set_answer(\"NRNkl\", int(np.mean(session_rewards)))\n",
    "    grader.submit(email, token)\n",
    "\n",
    "submit_cartpole(generate_session, 'fg3@williams.edu', 'h6Dxm7KXZiold3aF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
