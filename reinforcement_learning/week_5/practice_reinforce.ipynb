{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow\n",
    "\n",
    "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
    "\n",
    "It has been deliberately written to be as simple and human-readable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
    "\n",
    "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khairunn/Documents/DAT203/intro-to-dl/reinforcementLearning/week_3/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12485b650>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWyUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1220924d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "#gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env,'env'):\n",
    "    env=env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__. \n",
    "\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#create input variables. We only need <s,a,R> for REINFORCE\n",
    "states = tf.placeholder('float32',(None,)+state_dim,name=\"states\")\n",
    "actions = tf.placeholder('int32',name=\"action_ids\")\n",
    "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as L\n",
    "\n",
    "network = keras.models.Sequential()\n",
    "network.add(L.InputLayer(state_dim))\n",
    "network.add(L.Activation('linear'))\n",
    "\n",
    "network.add(L.Dense(200, activation='relu'))\n",
    "network.add(L.Dense(200, activation='relu'))\n",
    "\n",
    "network.add(L.Dense(n_actions))\n",
    "\n",
    "logits = network(states)\n",
    "\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function to pick action in one given state\n",
    "get_action_proba = lambda s: policy.eval({states:[s]})[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get probabilities for parti\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]),actions],axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy objective as in the last formula. please use mean, not sum.\n",
    "# note: you need to use log_policy_for_actions to get log probabilities for actions taken\n",
    "\n",
    "J = tf.reduce_mean(log_policy_for_actions * cumulative_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularize with entropy\n",
    "entropy = -tf.reduce_sum(policy * log_policy, 1, name='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "#weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
    "loss = -J -0.1 * entropy\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss,var_list=all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, #rewards at each step\n",
    "                           gamma = 0.99 #discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "    \n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    cumulative_rewards = np.zeros(len(rewards))\n",
    "    previous_reward = 0\n",
    "    \n",
    "    for i in range(len(rewards) + 1):\n",
    "        current_reward = rewards[-i] + gamma * previous_reward\n",
    "        cumulative_rewards[-i] = current_reward\n",
    "\n",
    "        previous_reward = current_reward\n",
    "        \n",
    "    return cumulative_rewards\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(_states,_actions,_rewards):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states:_states,actions:_actions,cumulative_rewards:_cumulative_rewards})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "    \n",
    "    #arrays to record session\n",
    "    states,actions,rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #action probabilities array aka pi(a|s)\n",
    "        action_probas = get_action_proba(s)\n",
    "        \n",
    "        a = np.random.choice(n_actions, 1, p=action_probas)[0]\n",
    "        # print(a)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    train_step(states,actions,rewards)\n",
    "            \n",
    "    return sum(rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:109.680\n",
      "mean reward:262.850\n",
      "mean reward:224.170\n",
      "mean reward:488.300\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-08 03:29:10,315] Making new env: CartPole-v0\n",
      "[2017-04-08 03:29:10,324] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-04-08 03:29:10,329] Clearing 6 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-08 03:29:10,336] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000000.mp4\n",
      "[2017-04-08 03:29:16,834] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000001.mp4\n",
      "[2017-04-08 03:29:23,689] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000008.mp4\n",
      "[2017-04-08 03:29:33,407] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000027.mp4\n",
      "[2017-04-08 03:29:45,840] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000064.mp4\n",
      "[2017-04-08 03:29:56,812] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/jheuristic/Downloads/sonnet/sonnet/examples/videos')\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.14221.video000027.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from submit import submit_cartpole\n",
    "submit_cartpole(generate_session, <EMAIL>, <TOKEN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# That's all, thank you for your attention!\n",
    "# Not having enough? There's an actor-critic waiting for you in the honor section.\n",
    "# But make sure you've seen the videos first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
